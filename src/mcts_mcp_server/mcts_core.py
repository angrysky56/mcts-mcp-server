#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Core MCTS Implementation
=======================

This module implements the Monte Carlo Tree Search (MCTS) algorithm
for advanced analysis and reasoning.
"""
import logging
import random
import math
import asyncio
import re
# import json # No longer used directly
# import os # No longer used directly
# import sqlite3 # No longer used directly
# from datetime import datetime # No longer used directly
from collections import namedtuple, Counter
from typing import (
    List, Optional, Dict, Any, Tuple, Set, Protocol, AsyncGenerator # Removed Union, Generator
)

import numpy as np

from .utils import (
    setup_logger,
    truncate_text,
    calculate_semantic_distance,
    _summarize_text,
    SKLEARN_AVAILABLE
)
# Note: TfidfVectorizer, ENGLISH_STOP_WORDS, cosine_similarity are now managed within utils.py
# If mcts_core directly needs them elsewhere, specific imports might be needed,
# but for now, they are primarily used by functions moved to utils.py.

from .mcts_config import DEFAULT_CONFIG, APPROACH_TAXONOMY, APPROACH_METADATA

# Initialize main logger for this module using the utility setup function
logger = setup_logger(__name__)

from .llm_interface import LLMInterface # Moved to its own file

# ==============================================================================
# MCTS Class
# ==============================================================================

from .node import Node
from .intent_handler import (
    IntentHandler,
    IntentResult,
    INITIAL_PROMPT,
    THOUGHTS_PROMPT,
    UPDATE_PROMPT,
    EVAL_ANSWER_PROMPT,
    TAG_GENERATION_PROMPT,
    FINAL_SYNTHESIS_PROMPT,
    INTENT_CLASSIFIER_PROMPT
)

# Define a simple structure to hold MCTS results
MCTSResult = namedtuple("MCTSResult", ["best_score", "best_solution_content", "mcts_instance"])

class MCTS:
    """Implements the Monte Carlo Tree Search algorithm for analysis."""

    def __init__(self,
                 llm_interface: LLMInterface,
                 question: str,
                 initial_analysis_content: str,
                 config: Optional[Dict[str, Any]] = None,
                 initial_state: Optional[Dict[str, Any]] = None):
        """
        Initializes the MCTS instance.

        Args:
            llm_interface: An object implementing the LLMInterface protocol.
            question: The original user question or text to analyze.
            initial_analysis_content: The initial analysis generated by the LLM.
            config: MCTS configuration dictionary (uses DEFAULT_CONFIG if None).
            initial_state: Optional dictionary containing state loaded from a previous run.
        """
        self.llm = llm_interface
        self.question = question
        self.config = config if config is not None else DEFAULT_CONFIG.copy()
        self.debug_logging = self.config.get("debug_logging", False)
        # Update logger level based on config (logger is now module-level)
        # The setup_logger in utils.py can be called again if needed,
        # or we can adjust the existing logger's level directly.
        # For now, the MCTS __init__ will set its own logger's level.
        # This assumes `logger` refers to the module logger created by `setup_logger(__name__)`
        if self.debug_logging:
            logger.setLevel(logging.DEBUG)
        else:
            logger.setLevel(logging.INFO)

        self.question_summary = _summarize_text(self.question, max_words=50) # Use imported _summarize_text
        self.loaded_initial_state = initial_state
        self.node_sequence = 0

        # Runtime state
        self.iterations_completed = 0
        self.simulations_completed = 0
        self.high_score_counter = 0 # For early stopping stability
        self.random_state = random.Random() # Use a dedicated random instance
        self.explored_approaches: Dict[str, List[str]] = {} # Track thoughts per approach type
        self.explored_thoughts: Set[str] = set() # Track unique thoughts generated
        self.approach_types: List[str] = ["initial"] # Track unique approach types encountered
        self.surprising_nodes: List[Node] = []
        self.memory: Dict[str, Any] = {"depth": 0, "branches": 0, "high_scoring_nodes": []}
        self.unfit_markers: List[Dict[str, Any]] = [] # Store unfit markers loaded/identified

        # --- Initialize Priors and Best Solution based on Loaded State ---
        prior_alpha = max(1e-9, self.config["beta_prior_alpha"])
        prior_beta = max(1e-9, self.config["beta_prior_beta"])

        # Approach Priors (for Bayesian mode)
        self.approach_alphas: Dict[str, float] = {}
        self.approach_betas: Dict[str, float] = {}
        initial_priors = self.loaded_initial_state.get("approach_priors") if self.loaded_initial_state else None
        if (self.config["use_bayesian_evaluation"] and initial_priors and
            isinstance(initial_priors.get("alpha"), dict) and
            isinstance(initial_priors.get("beta"), dict)):
            self.approach_alphas = {k: max(1e-9, v) for k, v in initial_priors["alpha"].items()}
            self.approach_betas = {k: max(1e-9, v) for k, v in initial_priors["beta"].items()}
            logger.info("Loaded approach priors from previous state.")
        else:
            # Initialize default priors for all known approaches + initial/variant
            all_approach_keys = list(APPROACH_TAXONOMY.keys()) + ["initial", "variant"]
            self.approach_alphas = {app: prior_alpha for app in all_approach_keys}
            self.approach_betas = {app: prior_beta for app in all_approach_keys}
            if self.config["use_bayesian_evaluation"]:
                 logger.info("Initialized default approach priors.")

        # Approach Scores (for non-Bayesian mode) - Simple average tracking
        self.approach_scores: Dict[str, float] = {} # Average score per approach

        # Best Solution Tracking
        self.best_score: float = 0.0
        self.best_solution: str = initial_analysis_content # Start with the initial analysis
        if self.loaded_initial_state:
            self.best_score = float(self.loaded_initial_state.get("best_score", 0.0))
            # Keep track of the previously best solution content if needed for context,
            # but the MCTS *starts* its search from the new initial_analysis_content.
            self.previous_best_solution_content = self.loaded_initial_state.get("best_solution_content")
            logger.info(f"Initialized best score ({self.best_score}) tracker from previous state.")
            # Load unfit markers
            self.unfit_markers = self.loaded_initial_state.get("unfit_markers", [])
            if self.unfit_markers:
                logger.info(f"Loaded {len(self.unfit_markers)} unfit markers from previous state.")


        # --- Initialize Root Node ---
        self.root = Node(
            content=initial_analysis_content,
            sequence=self.get_next_sequence(),
            parent=None,
            max_children=self.config["max_children"],
            use_bayesian_evaluation=self.config["use_bayesian_evaluation"],
            beta_prior_alpha=prior_alpha, # Root starts with default priors
            beta_prior_beta=prior_beta,
            approach_type="initial",
            approach_family="general"
        )
        # Initial simulation/backpropagation for the root node?
        # Not doing this in the original code, root starts with 0 visits/priors.

        logger.info(f"MCTS Initialized. Root Node Seq: {self.root.sequence}. Initial Best Score: {self.best_score:.2f}")
        if self.debug_logging:
             logger.debug(f"Initial Root Content: {truncate_text(self.root.content, 100)}")


    def get_next_sequence(self) -> int:
        """Gets the next sequential ID for a node."""
        self.node_sequence += 1
        return self.node_sequence

    # _summarize_text was moved to utils.py

    def get_context_for_node(self, node: Node) -> Dict[str, str]:
        """
        Gathers context for LLM prompts based on the current MCTS state and loaded state.
        Ensures all values are strings.
        """
        cfg = self.config
        best_answer_str = str(self.best_solution) if self.best_solution else "N/A"

        # --- Base Context ---
        context = {
            "question_summary": self.question_summary,
            "best_answer": truncate_text(best_answer_str, 300),
            "best_score": f"{self.best_score:.1f}",
            "current_answer": truncate_text(node.content, 300),
            "current_sequence": str(node.sequence),
            "current_approach": node.approach_type,
            "current_tags": ", ".join(node.descriptive_tags) if node.descriptive_tags else "None",
            "tree_depth": str(self.memory.get("depth", 0)),
            "branches": str(self.memory.get("branches", 0)),
            "approach_types": ", ".join(self.approach_types),
            # Initialize context from loaded state with defaults
            "previous_best_summary": "N/A",
            "unfit_markers_summary": "None",
            "learned_approach_summary": "Default priors",
            "explored_approaches": "None yet.",
            "high_scoring_examples": "None yet.",
            "sibling_approaches": "", # Default empty, populated below if applicable
        }

        # --- Context from Loaded State ---
        if self.loaded_initial_state:
            context["previous_best_summary"] = self.loaded_initial_state.get("best_solution_summary", "N/A")

            unfit = self.loaded_initial_state.get("unfit_markers", [])
            if unfit:
                markers_str = "; ".join([
                    f"'{m.get('summary', m.get('id', 'Unknown'))}' ({m.get('reason', 'Unknown')})"
                    for m in unfit[:5] # Show first 5
                ])
                context["unfit_markers_summary"] = markers_str + ("..." if len(unfit) > 5 else "")
            else:
                context["unfit_markers_summary"] = "None recorded"

            priors = self.loaded_initial_state.get("approach_priors")
            if priors and "alpha" in priors and "beta" in priors:
                means = {}
                for app, alpha in priors["alpha"].items():
                    beta = priors["beta"].get(app, 1.0)
                    alpha, beta = max(1e-9, alpha), max(1e-9, beta)
                    if alpha + beta > 1e-9: means[app] = (alpha / (alpha + beta)) * 10
                sorted_means = sorted(means.items(), key=lambda item: item[1], reverse=True)
                top_approaches = [f"{app} ({score:.1f})" for app, score in sorted_means[:3]]
                context["learned_approach_summary"] = f"Favors: {', '.join(top_approaches)}" + ("..." if len(sorted_means) > 3 else "")
            else:
                 context["learned_approach_summary"] = "Priors not loaded or incomplete"


        # --- Context from Current MCTS Run ---
        try: # Explored Thought Types (using current run's data)
            if cfg["track_explored_approaches"] and self.explored_approaches:
                exp_app_text = []
                current_alphas = self.approach_alphas
                current_betas = self.approach_betas
                sorted_approach_keys = sorted(self.explored_approaches.keys())

                for app in sorted_approach_keys:
                    thoughts = self.explored_approaches.get(app, [])
                    if thoughts:
                        count = len(thoughts)
                        score_text = ""
                        if cfg["use_bayesian_evaluation"]:
                            alpha = current_alphas.get(app, 1)
                            beta = current_betas.get(app, 1)
                            alpha, beta = max(1e-9, alpha), max(1e-9, beta)
                            if (alpha + beta) > 1e-9:
                                score_text = f"(β-Mean: {alpha / (alpha + beta):.2f}, N={count})"
                            else: score_text = f"(N={count})"
                        else:
                            score = self.approach_scores.get(app, 0) # Use simple avg score
                            count_non_bayes = sum(1 for n in self._find_nodes_by_approach(app) if n.visits > 0) # More accurate count?
                            if count_non_bayes > 0:
                                 score_text = f"(Avg: {score:.1f}, N={count_non_bayes})" # Use avg score if tracked
                            else: score_text = f"(N={count})"


                        sample_count = min(2, len(thoughts))
                        sample = thoughts[-sample_count:]
                        exp_app_text.append(f"- {app} {score_text}: {'; '.join([f'{truncate_text(str(t), 50)}' for t in sample])}")
                if exp_app_text: context["explored_approaches"] = "\n".join(exp_app_text)

        except Exception as e:
            logger.error(f"Ctx err (approaches): {e}")
            context["explored_approaches"] = "Error generating approach context."

        try: # High Scoring Examples
            if self.memory["high_scoring_nodes"]:
                high_score_text = [
                    f"- Score {score:.1f} ({app}): {truncate_text(content, 70)}"
                    for score, content, app, thought in self.memory["high_scoring_nodes"]
                ]
                context["high_scoring_examples"] = "\n".join(["Top Examples:"] + high_score_text)
        except Exception as e:
            logger.error(f"Ctx err (high scores): {e}")
            context["high_scoring_examples"] = "Error generating high score context."

        try: # Sibling Context
            if cfg["sibling_awareness"] and node.parent and len(node.parent.children) > 1:
                siblings = [c for c in node.parent.children if c is not None and c != node and c.visits > 0] # Only visited siblings
                if siblings:
                    sib_app_text = []
                    sorted_siblings = sorted(siblings, key=lambda s: s.sequence)
                    for s in sorted_siblings:
                        if s.thought: # Only show siblings that generated a thought
                            score = s.get_average_score()
                            tags_str = f"Tags: [{', '.join(s.descriptive_tags)}]" if s.descriptive_tags else ""
                            sib_app_text.append(f'"{truncate_text(str(s.thought), 50)}" -> (Score: {score:.1f} {tags_str})')
                    if sib_app_text:
                        context["sibling_approaches"] = "\n".join(["Siblings:"] + [f"- {sa}" for sa in sib_app_text])
        except Exception as e:
            logger.error(f"Ctx err (siblings): {e}")
            context["sibling_approaches"] = "Error generating sibling context."

        # Ensure all values are strings for final formatting
        safe_context = {k: str(v) if v is not None else "" for k, v in context.items()}
        return safe_context


    def _calculate_uct(self, node: Node, parent_visits: int) -> float:
        """Calculates the UCT score for a node, considering penalties and bonuses."""
        cfg = self.config
        if node.visits == 0:
            return float('inf') # Prioritize unvisited nodes

        # 1. Exploitation Term (normalized 0-1)
        exploitation = node.get_bayesian_mean() if cfg["use_bayesian_evaluation"] else (node.get_average_score() / 10.0)

        # 2. Exploration Term
        log_parent_visits = math.log(max(1, parent_visits))
        exploration = cfg["exploration_weight"] * math.sqrt(log_parent_visits / node.visits)

        # 3. Penalty for Unfit Nodes (using loaded/identified markers)
        penalty = 0.0
        is_unfit = False
        if self.unfit_markers:
            node_summary = node.thought or node.content # Use thought if available, else content
            node_tags_set = set(node.descriptive_tags)
            for marker in self.unfit_markers:
                # Quick checks first
                if marker.get("id") == node.id or marker.get("sequence") == node.sequence:
                    is_unfit = True; break
                # Check tag overlap
                marker_tags_set = set(marker.get('tags', []))
                if node_tags_set and marker_tags_set and len(node_tags_set.intersection(marker_tags_set)) > 0:
                    is_unfit = True; break # Simple tag overlap check

            # Apply penalty if unfit and *not* surprising (allow surprise to override)
            if is_unfit and not node.is_surprising:
                penalty = -100.0 # Strong penalty to avoid selecting unfit nodes
                if self.debug_logging: logger.debug(f"Applying UCT penalty to unfit Node {node.sequence}")


        # 4. Surprise Bonus
        surprise_bonus = 0.3 if node.is_surprising else 0.0 # Simple fixed bonus

        # 5. Diversity Bonus (relative to siblings)
        diversity_bonus = 0.0
        if node.parent and len(node.parent.children) > 1 and cfg["score_diversity_bonus"] > 0:
            my_score_norm = node.get_average_score() / 10.0
            sibling_scores = [
                (sib.get_average_score() / 10.0)
                for sib in node.parent.children
                if sib is not None and sib != node and sib.visits > 0
            ]
            if sibling_scores:
                sibling_avg = sum(sibling_scores) / len(sibling_scores)
                diversity_bonus = cfg["score_diversity_bonus"] * abs(my_score_norm - sibling_avg)

        # Combine terms
        uct_value = exploitation + exploration + surprise_bonus + diversity_bonus + penalty
        # Ensure finite return, default to low value if not
        return uct_value if math.isfinite(uct_value) else -float('inf')

    def _collect_non_leaf_nodes(self, node: Node, non_leaf_nodes: List[Node], max_depth: int, current_depth: int = 0):
        """Helper to find nodes that can still be expanded within a depth limit."""
        if current_depth > max_depth or node is None:
            return
        # Node is non-leaf if it HAS children AND is not fully expanded yet
        if node.children and not node.fully_expanded():
            non_leaf_nodes.append(node)

        for child in node.children:
             # Recursive call only if child exists
            if child is not None:
                self._collect_non_leaf_nodes(child, non_leaf_nodes, max_depth, current_depth + 1)


    async def select(self) -> Node:
        """Selects a node for expansion using UCT or Thompson Sampling."""
        cfg = self.config
        node = self.root
        selection_path_ids = [node.id] # Track path by ID

        # Optional: Branch Enhancement (Force exploration of less visited branches)
        force_interval = cfg["force_exploration_interval"]
        if (force_interval > 0 and self.simulations_completed > 0 and
            self.simulations_completed % force_interval == 0 and self.memory["depth"] > 1):
            candidate_nodes = []
            # Collect expandable nodes up to half the current max depth
            self._collect_non_leaf_nodes(self.root, candidate_nodes, max_depth=max(1, self.memory["depth"] // 2))
            expandable_candidates = [n for n in candidate_nodes if not n.fully_expanded()]
            if expandable_candidates:
                forced_node = self.random_state.choice(expandable_candidates)
                if self.debug_logging: logger.debug(f"BRANCH ENHANCE: Forcing selection of Node {forced_node.sequence}")
                # Need to return the actual node selected by force
                return forced_node # Exit selection early with the forced node

        # Standard Selection Loop
        while node.children: # While the node has children listed
            valid_children = [child for child in node.children if child is not None]
            if not valid_children:
                 logger.warning(f"Node {node.sequence} has empty children list or only None entries. Stopping selection.")
                 break # Cannot proceed

            parent_visits = node.visits
            unvisited = [child for child in valid_children if child.visits == 0]

            if unvisited:
                selected_child = self.random_state.choice(unvisited)
                node = selected_child # Move to the unvisited child
                selection_path_ids.append(node.id)
                break # Stop selection here, this node will be expanded/simulated

            # If all children visited, use selection strategy
            if cfg["use_thompson_sampling"] and cfg["use_bayesian_evaluation"]:
                # Thompson Sampling
                samples = [(child, child.thompson_sample()) for child in valid_children]
                if not samples:
                    logger.warning(f"No valid Thompson samples for children of {node.sequence}. Selecting randomly.")
                    selected_child = self.random_state.choice(valid_children)
                else:
                    selected_child, _ = max(samples, key=lambda x: x[1])
                node = selected_child
            else:
                # UCT Selection
                uct_values = []
                for child in valid_children:
                     try:
                         uct = self._calculate_uct(child, parent_visits)
                         if math.isfinite(uct): uct_values.append((child, uct))
                         else: logger.warning(f"UCT for child {child.sequence} was non-finite. Skipping.")
                     except Exception as uct_err:
                         logger.error(f"UCT calculation error for node {child.sequence}: {uct_err}")

                if not uct_values:
                    logger.warning(f"No valid UCT values for children of {node.sequence}. Selecting randomly.")
                    if not valid_children: # Should not happen if loop condition met, but safety check
                         logger.error(f"Selection error: Node {node.sequence} has no valid children. Cannot proceed.")
                         return node # Return current node as selection cannot advance
                    selected_child = self.random_state.choice(valid_children)
                else:
                    uct_values.sort(key=lambda x: x[1], reverse=True) # Highest UCT wins
                    selected_child = uct_values[0][0]
                node = selected_child

            selection_path_ids.append(node.id) # Add selected node to path

            # If the newly selected node is not fully expanded, stop selection (it's the target)
            # Or if it has no children (it's a leaf node)
            if not node.fully_expanded() or not node.children:
                break

        # Update max depth seen
        current_depth = len(selection_path_ids) - 1
        self.memory["depth"] = max(self.memory.get("depth", 0), current_depth)
        if self.debug_logging:
             path_seq = [(node.sequence if node else '?') for nid in selection_path_ids for node in [self._find_node_by_id(nid)]]
             logger.debug(f"Selection path (Sequences): {' -> '.join(map(str, path_seq))}")

        return node # Return the selected leaf or expandable node


    def _classify_approach(self, thought: str) -> Tuple[str, str]:
        """Classifies the thought into an approach type and family using keywords."""
        approach_type = "variant" # Default if no keywords match
        approach_family = "general"
        if not thought or not isinstance(thought, str):
            return approach_type, approach_family

        thought_lower = thought.lower()
        approach_scores = {
            app: sum(1 for kw in kws if kw in thought_lower)
            for app, kws in APPROACH_TAXONOMY.items() if kws # Check if keywords exist
        }
        positive_scores = {app: score for app, score in approach_scores.items() if score > 0}

        if positive_scores:
            max_score = max(positive_scores.values())
            # Handle ties by random choice among best
            best_approaches = [app for app, score in positive_scores.items() if score == max_score]
            approach_type = self.random_state.choice(best_approaches)

        # Get family from metadata
        approach_family = APPROACH_METADATA.get(approach_type, {}).get("family", "general")

        if self.debug_logging:
            logger.debug(f"Classified thought '{truncate_text(thought, 50)}' as: {approach_type} ({approach_family})")
        return approach_type, approach_family

    def _check_surprise(self, parent_node: Node, new_content: str, new_approach_type: str, new_approach_family: str) -> Tuple[bool, str]:
        """Checks if the new node content/approach is surprising relative to the parent."""
        cfg = self.config
        surprise_factors = []
        is_surprising = False
        surprise_explanation = ""

        # 1. Semantic Distance Check
        if cfg["use_semantic_distance"]:
            try:
                parent_content = str(parent_node.content) if parent_node.content else ""
                new_content_str = str(new_content) if new_content else ""
                if parent_content and new_content_str:
                    dist = calculate_semantic_distance(parent_content, new_content_str, use_tfidf=True) # Can disable TFIDF here if too slow
                    if dist > cfg["surprise_threshold"]:
                        surprise_factors.append({
                            "type": "semantic", "value": dist,
                            "weight": cfg["surprise_semantic_weight"],
                            "desc": f"Semantic dist ({dist:.2f})"
                        })
            except Exception as e: logger.warning(f"Semantic distance check failed: {e}")

        # 2. Shift in Thought Approach Family
        parent_family = parent_node.approach_family
        if parent_family != new_approach_family and new_approach_family != "general":
            surprise_factors.append({
                "type": "family_shift", "value": 1.0,
                "weight": cfg["surprise_philosophical_shift_weight"],
                "desc": f"Shift '{parent_family}'->'{new_approach_family}'"
            })

        # 3. Novelty of Thought Approach Family (using BFS on current tree)
        try:
            family_counts = Counter()
            queue = [(self.root, 0)] if self.root else []
            processed_bfs = set()
            nodes_visited = 0
            MAX_BFS_NODES = 100
            MAX_BFS_DEPTH = 5

            while queue and nodes_visited < MAX_BFS_NODES:
                curr_node, depth = queue.pop(0)
                if curr_node is None or curr_node.id in processed_bfs or depth > MAX_BFS_DEPTH:
                    continue
                processed_bfs.add(curr_node.id)
                nodes_visited += 1
                family_counts[curr_node.approach_family] += 1
                if depth + 1 <= MAX_BFS_DEPTH:
                    queue.extend([(child, depth + 1) for child in curr_node.children if child is not None])

            # If the new family has been seen <= 1 times (itself) and isn't 'general'
            if family_counts.get(new_approach_family, 0) <= 1 and new_approach_family != "general":
                surprise_factors.append({
                    "type": "novelty", "value": 0.8, # Slightly less value than shift/semantic maybe?
                    "weight": cfg["surprise_novelty_weight"],
                    "desc": f"Novel approach family ('{new_approach_family}')"
                })
        except Exception as e: logger.warning(f"Novelty check BFS failed: {e}", exc_info=self.debug_logging)


        # Calculate combined weighted score
        if surprise_factors:
            total_weighted_score = sum(f["value"] * f["weight"] for f in surprise_factors)
            total_weight = sum(f["weight"] for f in surprise_factors)
            combined_score = (total_weighted_score / total_weight) if total_weight > 1e-6 else 0.0

            if combined_score >= cfg["surprise_overall_threshold"]:
                is_surprising = True
                factor_descs = [f"- {f['desc']} (Val:{f['value']:.2f}, W:{f['weight']:.1f})" for f in surprise_factors]
                surprise_explanation = (f"Combined surprise ({combined_score:.2f} >= {cfg['surprise_overall_threshold']}):\n" + "\n".join(factor_descs))
                if self.debug_logging: logger.debug(f"Surprise DETECTED for node sequence {parent_node.sequence+1}: Score={combined_score:.2f}\n{surprise_explanation}")

        return is_surprising, surprise_explanation


    async def expand(self, node: Node) -> Optional[Node]:
        """Expands a node by generating a thought and a new analysis."""
        cfg = self.config
        if node.fully_expanded():
             logger.warning(f"Attempted to expand fully expanded Node {node.sequence}. Returning None.")
             return None
        if not node.content:
             logger.warning(f"Attempted to expand Node {node.sequence} with no content. Returning None.")
             return None

        try:
            context = self.get_context_for_node(node)

            # 1. Generate Thought
            if self.debug_logging: logger.debug(f"Generating thought for Node {node.sequence}")
            thought = await self.llm.generate_thought(context, cfg)
            if not isinstance(thought, str) or not thought.strip() or "Error:" in thought:
                logger.error(f"Invalid thought generation for Node {node.sequence}: '{thought}'")
                return None # Expansion failed
            thought = thought.strip()
            if self.debug_logging: logger.debug(f"Node {node.sequence} -> Thought: '{truncate_text(thought, 80)}'")

            # Check thought against unfit markers (simple check)
            is_unfit_thought = False
            if self.unfit_markers:
                 for marker in self.unfit_markers:
                      marker_summary = marker.get('summary')
                      # if marker_summary and calculate_semantic_distance(thought, marker_summary) < 0.15: # Strict threshold
                      #      is_unfit_thought = True
                      #      logger.warning(f"Generated thought for Node {node.sequence} resembles unfit marker '{marker_summary}'. Skipping expansion.")
                      #      break
            if is_unfit_thought: return None # Skip expansion if thought seems unfit

            # Classify approach based on thought
            approach_type, approach_family = self._classify_approach(thought)
            self.explored_thoughts.add(thought)
            if approach_type not in self.approach_types: self.approach_types.append(approach_type)
            if approach_type not in self.explored_approaches: self.explored_approaches[approach_type] = []
            self.explored_approaches[approach_type].append(thought)


            # 2. Update Analysis based on Thought
            if self.debug_logging: logger.debug(f"Updating analysis for Node {node.sequence} based on thought")
            # Pass original content in context for update prompt
            context_for_update = context.copy()
            context_for_update['answer'] = node.content # Use 'answer' key as expected by UPDATE_PROMPT
            context_for_update['improvements'] = thought # Use 'improvements' key
            new_content = await self.llm.update_analysis(thought, context_for_update, cfg)

            if not isinstance(new_content, str) or not new_content.strip() or "Error:" in new_content:
                logger.error(f"Invalid new content generation for Node {node.sequence}: '{new_content}'")
                return None # Expansion failed
            new_content = new_content.strip()
            if self.debug_logging: logger.debug(f"Node {node.sequence} -> New Content: '{truncate_text(new_content, 80)}'")

            # 3. Generate Tags for New Content
            new_tags = await self.llm.generate_tags(new_content, cfg)
            if self.debug_logging: logger.debug(f"Generated Tags for new node: {new_tags}")

            # 4. Check for Surprise
            is_surprising, surprise_explanation = self._check_surprise(node, new_content, approach_type, approach_family)

            # 5. Create Child Node
            child = Node(
                content=new_content,
                parent=node,
                sequence=self.get_next_sequence(),
                thought=thought,
                approach_type=approach_type,
                approach_family=approach_family,
                max_children=cfg["max_children"],
                use_bayesian_evaluation=cfg["use_bayesian_evaluation"],
                beta_prior_alpha=cfg["beta_prior_alpha"], # Child starts with default priors
                beta_prior_beta=cfg["beta_prior_beta"]
            )
            child.descriptive_tags = new_tags
            child.is_surprising = is_surprising
            child.surprise_explanation = surprise_explanation

            # Add child to parent
            node.add_child(child)
            if is_surprising: self.surprising_nodes.append(child)

            # Update branch count if this adds a new branch
            if len(node.children) > 1: self.memory["branches"] += 1

            if self.debug_logging: logger.debug(f"Successfully expanded Node {node.sequence} -> Child {child.sequence}")
            return child

        except Exception as e:
            logger.error(f"Expand error on Node {node.sequence}: {e}", exc_info=self.debug_logging)
            return None


    async def simulate(self, node: Node) -> Optional[float]:
        """Simulates (evaluates) a node using the LLM, returns score (1-10)."""
        cfg = self.config
        if not node.content:
            logger.warning(f"Cannot simulate Node {node.sequence}: Content is empty. Returning default score 5.0")
            return 5.0

        try:
            context = self.get_context_for_node(node)
            # Ensure context has the key expected by the eval prompt
            context['answer_to_evaluate'] = node.content

            if self.debug_logging: logger.debug(f"Evaluating Node {node.sequence}")
            raw_score = await self.llm.evaluate_analysis(node.content, context, cfg)

            # Validate score is int 1-10
            if not isinstance(raw_score, int) or not (1 <= raw_score <= 10):
                logger.error(f"Evaluation for Node {node.sequence} returned invalid score: {raw_score}. Defaulting to 5.")
                raw_score = 5

            score = float(raw_score)
            node.raw_scores.append(raw_score) # Keep track of raw scores received
            approach = node.approach_type if node.approach_type else "unknown"

            # Update approach performance tracking
            if cfg["use_bayesian_evaluation"]:
                 # Use raw score for pseudo counts (scale 1-10)
                 pseudo_successes = max(0, raw_score - 1) # 10 -> 9 successes
                 pseudo_failures = max(0, 10 - raw_score) # 3 -> 7 failures
                 # Ensure approach exists in prior dicts, initializing if necessary
                 current_alpha = self.approach_alphas.setdefault(approach, cfg["beta_prior_alpha"])
                 current_beta = self.approach_betas.setdefault(approach, cfg["beta_prior_beta"])
                 # Update priors safely
                 self.approach_alphas[approach] = max(1e-9, current_alpha + pseudo_successes)
                 self.approach_betas[approach] = max(1e-9, current_beta + pseudo_failures)
            else:
                 # Non-Bayesian: Update simple average score (e.g., using EMA)
                 current_avg = self.approach_scores.get(approach, score) # Initialize with current score if first time
                 self.approach_scores[approach] = 0.7 * score + 0.3 * current_avg # EMA update

            if self.debug_logging: logger.debug(f"Node {node.sequence} evaluation result: {score:.1f}/10")

            # Update high score memory (use score 1-10)
            if score >= 7:
                entry = (score, node.content, approach, node.thought)
                self.memory["high_scoring_nodes"].append(entry)
                # Sort and trim memory
                self.memory["high_scoring_nodes"].sort(key=lambda x: x[0], reverse=True)
                self.memory["high_scoring_nodes"] = self.memory["high_scoring_nodes"][:cfg["memory_cutoff"]]

            return score

        except Exception as e:
            logger.error(f"Simulate error for Node {node.sequence}: {e}", exc_info=self.debug_logging)
            return None # Indicate simulation failure

    def backpropagate(self, node: Node, score: float):
        """Backpropagates the simulation score up the tree."""
        cfg = self.config
        if score is None or not math.isfinite(score):
             logger.warning(f"Invalid score ({score}) received for backpropagation from Node {node.sequence}. Skipping.")
             return

        if self.debug_logging: logger.debug(f"Backpropagating score {score:.2f} from Node {node.sequence}")

        # Normalize score to 0-1 for Bayesian updates if score is 1-10
        normalized_score = score / 10.0
        pseudo_successes = max(0, score - 1) # Use 1-10 score for pseudo counts
        pseudo_failures = max(0, 10 - score)

        temp_node: Optional[Node] = node
        path_len = 0
        while temp_node:
            temp_node.visits += 1
            if cfg["use_bayesian_evaluation"]:
                 if temp_node.alpha is not None and temp_node.beta is not None:
                     # Update using pseudo counts from 1-10 score
                     temp_node.alpha = max(1e-9, temp_node.alpha + pseudo_successes)
                     temp_node.beta = max(1e-9, temp_node.beta + pseudo_failures)
                 else: logger.warning(f"Node {temp_node.sequence} missing alpha/beta during backprop.")
            else: # Non-Bayesian: Add score to cumulative value
                 if temp_node.value is not None:
                     temp_node.value += score # Add the raw score (1-10)
                 else: # Initialize if missing (should only happen for root if not pre-simulated)
                      logger.warning(f"Node {temp_node.sequence} missing value during non-Bayesian backprop. Initializing.")
                      temp_node.value = score

            temp_node = temp_node.parent
            path_len += 1

        if self.debug_logging: logger.debug(f"Backpropagation complete for Node {node.sequence} (Path length: {path_len})")

    async def run_search_iterations(self, num_iterations: int, simulations_per_iteration: int) -> None:
        """Runs the main MCTS search loop."""
        cfg = self.config
        logger.info(f"Starting MCTS search: {num_iterations} iterations, {simulations_per_iteration} simulations/iter.")

        # Performance optimization - run multiple simulations concurrently
        max_concurrent = 3  # Set a reasonable limit for concurrency

        for i in range(num_iterations):
            self.iterations_completed = i + 1
            logger.info(f"--- Starting Iteration {self.iterations_completed}/{num_iterations} ---")
            best_score_before_iter = self.best_score

            # Process simulations in batches for better concurrency
            for batch_start in range(0, simulations_per_iteration, max_concurrent):
                batch_size = min(max_concurrent, simulations_per_iteration - batch_start)
                batch_tasks = []

                # Create tasks for the batch
                for j in range(batch_start, batch_start + batch_size):
                    sim_num = j + 1
                    task = asyncio.create_task(self._run_single_simulation(sim_num, simulations_per_iteration))
                    batch_tasks.append(task)

                # Wait for the batch to complete
                await asyncio.gather(*batch_tasks)

                # Check early stopping after each batch
                if (cfg["early_stopping"] and
                    self.best_score >= cfg["early_stopping_threshold"] and
                    self.high_score_counter >= cfg["early_stopping_stability"]):
                    logger.info(f"EARLY STOPPING criteria met during Iteration {self.iterations_completed}.")
                    return  # Exit early

            # --- End of Simulations for Iteration i ---
            logger.info(f"--- Finished Iteration {self.iterations_completed}. Current Best Score: {self.best_score:.2f} ---")

            # Re-check early stopping condition after the iteration
            if (cfg["early_stopping"] and
                self.best_score >= cfg["early_stopping_threshold"] and
                self.high_score_counter >= cfg["early_stopping_stability"]):
                logger.info(f"EARLY STOPPING criteria met at end of Iteration {self.iterations_completed}.")
                break  # Exit outer iteration loop

        logger.info("MCTS search finished.")

    async def _run_single_simulation(self, current_sim_num: int, total_sims: int) -> None:
        """Runs a single simulation (select-expand-simulate-backpropagate cycle)."""
        self.simulations_completed += 1
        cfg = self.config

        if self.debug_logging:
            logger.debug(f"--- Sim {current_sim_num}/{total_sims} ---")

        # 1. Select
        leaf = await self.select()
        if not leaf:
            logger.error(f"Sim {current_sim_num}: Selection returned None. Skipping simulation.")
            return

        # 2. Expand (if not terminal and not fully expanded)
        node_to_simulate = leaf
        if not leaf.fully_expanded() and leaf.content:  # Check content exists
            if self.debug_logging:
                logger.debug(f"Sim {current_sim_num}: Attempting expansion from Node {leaf.sequence}")
            expanded_node = await self.expand(leaf)
            if expanded_node:
                node_to_simulate = expanded_node  # Simulate the newly expanded node
                if self.debug_logging:
                    logger.debug(f"Sim {current_sim_num}: Expanded {leaf.sequence} -> {node_to_simulate.sequence}")
            else:
                if self.debug_logging:
                    logger.warning(f"Sim {current_sim_num}: Expansion failed for {leaf.sequence}. Simulating original leaf.")
                node_to_simulate = leaf  # Simulate original leaf if expansion failed
        elif self.debug_logging:
            logger.debug(f"Sim {current_sim_num}: Node {leaf.sequence} is fully expanded or has no content. Simulating directly.")

        # 3. Simulate
        score = None
        if node_to_simulate and node_to_simulate.content:
            score = await self.simulate(node_to_simulate)
        elif node_to_simulate:
            logger.warning(f"Sim {current_sim_num}: Skipping simulation for {node_to_simulate.sequence} (no content).")
            score = 5.0  # Assign default score
        else:  # Should not happen if selection worked
            logger.error(f"Sim {current_sim_num}: node_to_simulate is None after select/expand. Skipping simulation.")
            return  # Skip backprop

        # 4. Backpropagate
        if score is not None:
            self.backpropagate(node_to_simulate, score)

            # Update overall best score/solution found so far
            if score > self.best_score:
                logger.info(f"Sim {current_sim_num}: ✨ New best! Score: {score:.1f} (Node {node_to_simulate.sequence})")
                self.best_score = score
                self.best_solution = str(node_to_simulate.content)
                self.high_score_counter = 0  # Reset stability counter
            elif score == self.best_score:
                # If score matches best, don't reset counter
                pass
            else:  # Score is lower than best
                self.high_score_counter = 0  # Reset stability counter if score drops

            # Check early stopping (threshold) - based on overall best score
            if cfg["early_stopping"] and self.best_score >= cfg["early_stopping_threshold"]:
                self.high_score_counter += 1  # Increment counter only if score >= threshold
                if self.debug_logging:
                    logger.debug(f"Sim {current_sim_num}: Best score ({self.best_score:.1f}) >= threshold. Stability: {self.high_score_counter}/{cfg['early_stopping_stability']}")
        else:  # Simulation failed (score is None)
            if node_to_simulate:
                logger.warning(f"Sim {current_sim_num}: Simulation failed for Node {node_to_simulate.sequence}. No score obtained.")
            self.high_score_counter = 0  # Reset stability counter if sim fails


    def get_final_results(self) -> MCTSResult:
        """Returns the best score and solution found."""
        # Clean the best solution content of <think> tags if present
        cleaned_solution = self.best_solution
        if cleaned_solution and isinstance(cleaned_solution, str):
            # First try to remove the entire <think> block if it's a pure think block
            clean_attempt = re.sub(r'<think>.*?</think>', '', cleaned_solution, flags=re.DOTALL)
            # If that removes everything, keep the original but strip just the tags
            if not clean_attempt.strip() and ("<think>" in cleaned_solution or "</think>" in cleaned_solution):
                cleaned_solution = re.sub(r'</?think>', '', cleaned_solution)
            else:
                cleaned_solution = clean_attempt

        # In a real app, you might want more detailed results (e.g., best node path)
        return MCTSResult(
            best_score=self.best_score,
            best_solution_content=cleaned_solution.strip() if isinstance(cleaned_solution, str) else cleaned_solution,
            mcts_instance=self # Return self for further analysis if needed
        )

    def find_best_final_node(self) -> Optional[Node]:
        """Finds the node object corresponding to the best solution content."""
        if not self.best_solution or not self.root: return None

        queue = [self.root]
        visited_ids = {self.root.id}
        best_match_node = None
        min_score_diff = float('inf')

        # Clean target solution content once
        target_content = str(self.best_solution).strip()
        target_content = re.sub(r"^```(json|markdown)?\s*", "", target_content, flags=re.IGNORECASE | re.MULTILINE)
        target_content = re.sub(r"\s*```$", "", target_content, flags=re.MULTILINE).strip()

        while queue:
            current = queue.pop(0)
            if current is None: continue

            # Clean node content for comparison
            node_content = str(current.content).strip()
            node_content = re.sub(r"^```(json|markdown)?\s*", "", node_content, flags=re.IGNORECASE | re.MULTILINE)
            node_content = re.sub(r"\s*```$", "", node_content, flags=re.MULTILINE).strip()

            # Check for exact content match (after cleaning)
            if node_content == target_content:
                score_diff = abs(current.get_average_score() - self.best_score)
                # Prefer node with score closest to the recorded best score
                if best_match_node is None or score_diff < min_score_diff:
                    best_match_node = current
                    min_score_diff = score_diff
                    # Optimization: If perfect match found, can stop early? Only if content is guaranteed unique.
                    # Let's keep searching to ensure we find the one with the closest score if duplicates exist.

            # Add valid children to queue
            for child in current.children:
                if child and child.id not in visited_ids:
                    visited_ids.add(child.id)
                    queue.append(child)

        if not best_match_node:
            logger.warning("Could not find node object exactly matching best solution content. Best score might be from a pruned or non-existent node state.")
            # Fallback: Find node with highest score overall?
            # best_overall_node = self._find_node_with_highest_score()
            # return best_overall_node
            # For now, return None if exact content match fails.
            return None

        return best_match_node

    def _find_node_by_id(self, node_id: str) -> Optional[Node]:
        """Finds a node by its ID using BFS."""
        if not self.root: return None
        queue = [self.root]
        visited = {self.root.id}
        while queue:
            current = queue.pop(0)
            if current.id == node_id:
                return current
            for child in current.children:
                if child and child.id not in visited:
                     visited.add(child.id)
                     queue.append(child)
        return None

    def _find_nodes_by_approach(self, approach_type: str) -> List[Node]:
        """Finds all nodes with a specific approach type using BFS."""
        nodes = []
        if not self.root: return nodes
        queue = [self.root]
        visited = {self.root.id}
        while queue:
             current = queue.pop(0)
             if current.approach_type == approach_type:
                  nodes.append(current)
             for child in current.children:
                  if child and child.id not in visited:
                       visited.add(child.id)
                       queue.append(child)
        return nodes


    def export_tree_summary(self) -> Dict:
         """Exports a summary of the tree structure and key nodes."""
         if not self.root: return {"error": "No root node"}
         return self.root.node_to_json() # Use the recursive JSON export

    def get_best_path_nodes(self) -> List[Node]:
         """Traces the path from the root to the best scoring node found."""
         best_node = self.find_best_final_node()
         if not best_node: return []
         path = []
         current = best_node
         while current:
              path.append(current)
              current = current.parent
         return path[::-1] # Reverse to get root -> best order

# ==============================================================================
# Intent Handling
# ==============================================================================

# StateManager import moved to the top with other local imports
